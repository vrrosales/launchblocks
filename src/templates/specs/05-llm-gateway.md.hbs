# Module 5: LLM Gateway

> Spec for **{{app_name}}** -- generated by Launchblocks

## Purpose

The LLM Gateway is the single entry point for **every** LLM call in the application. No feature code, page, component, or API route should ever import or call a provider SDK directly. All LLM interactions flow through the gateway, which handles provider abstraction, authorization, audit logging, streaming, error handling, and cost tracking in one place.

This architecture ensures that switching providers, adding rate limits, or changing logging behavior requires changes in exactly one module rather than scattered across the codebase.

## Architecture Overview

```
Feature Code (pages, components, server actions)
        |
        v
  POST /api/llm/chat   <-- single HTTP endpoint
        |
        v
  LLM Gateway Service
   ├── Authorization check (role-based)
   ├── Template resolution (if slug provided)
   ├── Variable interpolation
   ├── Provider routing
   ├── Token counting
   ├── Cost estimation
   ├── Audit logging (writes to llm_audit_log)
   └── Error handling
        |
        v
  Provider Abstraction Layer
   {{#each providers_display}}
   ├── {{this.name}} Adapter
   {{/each}}
```

**Rule:** Import the gateway client utility, never a provider SDK, in feature code.

## Provider Abstraction Layer

Each provider adapter implements a unified `LLMProvider` interface:

```typescript
interface LLMProvider {
  id: string;                    // "{{providers_display.[0].id}}"{{#if has_multiple_providers}}, etc.{{/if}}
  chat(request: LLMRequest): Promise<LLMResponse>;
  chatStream(request: LLMRequest): AsyncIterable<LLMStreamChunk>;
  countTokens(text: string, model: string): number;
  listModels(): string[];
}

interface LLMRequest {
  model: string;
  messages: ChatMessage[];
  temperature?: number;          // 0-2, default 0.7
  max_tokens?: number;           // default 1024
  stop_sequences?: string[];
  metadata?: Record<string, unknown>;
}

interface ChatMessage {
  role: "system" | "user" | "assistant";
  content: string;
}

interface LLMResponse {
  content: string;
  model: string;
  provider: string;
  input_tokens: number;
  output_tokens: number;
  latency_ms: number;
  finish_reason: "stop" | "length" | "error";
}

interface LLMStreamChunk {
  content: string;
  done: boolean;
  input_tokens?: number;         // present on final chunk
  output_tokens?: number;        // present on final chunk
}
```

### Supported Providers

{{#each providers_display}}
- **{{this.name}}** (`{{this.id}}`) -- adapter in `lib/llm/providers/{{this.id}}.ts`
{{/each}}

{{#if has_multiple_providers}}
The gateway selects the correct adapter based on the `provider` field in the request or the provider configured on the prompt template. If no provider is specified, the first configured provider (`{{providers_display.[0].id}}`) is used as the default.
{{else}}
This project uses a single provider (`{{providers_display.[0].id}}`). The abstraction layer is still in place so additional providers can be added later without changing feature code.
{{/if}}

## Gateway API

### `POST /api/llm/chat`

This is the only endpoint feature code should call for LLM interactions.

#### Request Body -- Template-Based Call

Use a template-based call when referencing a saved prompt template by its slug. The gateway resolves the template, interpolates variables, and sends the assembled messages to the provider.

```json
{
  "template_slug": "summarize-article",
  "variables": {
    "article_text": "The full article content here...",
    "max_length": "3 sentences"
  },
  "stream": false
}
```

#### Request Body -- Ad-Hoc Call

Use an ad-hoc call when feature code needs to send raw prompts without a saved template. The caller specifies the messages, model, and provider directly.

```json
{
  "messages": [
    { "role": "system", "content": "You are a helpful assistant." },
    { "role": "user", "content": "Explain quantum computing in simple terms." }
  ],
  "provider": "{{providers_display.[0].id}}",
  "model": "gpt-4o",
  "temperature": 0.7,
  "max_tokens": 1024,
  "stream": false
}
```

#### Success Response (non-streaming)

```json
{
  "content": "The generated response text...",
  "model": "gpt-4o",
  "provider": "{{providers_display.[0].id}}",
  "usage": {
    "input_tokens": 142,
    "output_tokens": 287,
    "total_tokens": 429
  },
  "estimated_cost": 0.003215,
  "latency_ms": 1823,
  "audit_id": "uuid-of-the-audit-log-entry"
}
```

#### Streaming Response

When `"stream": true`, the endpoint returns a `text/event-stream` response using Server-Sent Events (SSE):

```
data: {"content": "The ", "done": false}
data: {"content": "generated ", "done": false}
data: {"content": "response.", "done": false}
data: {"content": "", "done": true, "usage": {"input_tokens": 142, "output_tokens": 12, "total_tokens": 154}, "estimated_cost": 0.001, "latency_ms": 890, "audit_id": "uuid"}
```

The final chunk (`done: true`) includes the full usage summary. The client should accumulate content chunks and read the usage from the final event.

## Authorization

Only users whose role is in the LLM access list can call the gateway.

**Authorized roles:** {{llm_access_roles_joined}}

{{#if all_roles_have_llm}}
All roles in this project have LLM access, so every authenticated and approved user can make calls.
{{else}}
Not all roles have LLM access. The gateway must check the user's role before processing any request.
{{/if}}

Implementation:

```typescript
// In the API route handler
const user = await getAuthenticatedUser(request);
if (!user || user.status !== "approved") {
  return Response.json({ error: "Unauthorized" }, { status: 401 });
}

const allowedRoles = [{{#each llm_access_roles}}"{{this}}"{{#unless @last}}, {{/unless}}{{/each}}];
if (!allowedRoles.includes(user.role)) {
  return Response.json({ error: "Your role does not have LLM access" }, { status: 403 });
}
```

## Template Resolution

When a request includes `template_slug`, the gateway:

1. Queries `prompt_templates` by slug where `is_active = true`
2. Returns 404 if the template does not exist or is inactive
3. Reads the template's `system_prompt`, `user_prompt_template`, `provider`, `model`, `temperature`, and `max_tokens`
4. Interpolates `\{{variable_name}}` placeholders in both prompts using the provided `variables` object
5. Assembles the messages array: system message from `system_prompt`, user message from the interpolated `user_prompt_template`
6. Sends the assembled request to the correct provider adapter

Variable interpolation uses simple string replacement. Any `\{{key}}` in the template that does not have a matching key in `variables` should be left as-is (do not throw an error, as some templates may include optional variables).

## Audit Logging

Every call through the gateway -- successful or failed -- is logged to the `llm_audit_log` table. The gateway writes the audit entry **after** the provider responds (or after an error), using the Supabase service role client to bypass RLS.

Fields logged per call:

| Field | Source |
|---|---|
| `user_id` | Authenticated user's ID |
| `prompt_template_id` | UUID of the template (null for ad-hoc calls) |
| `provider` | Provider ID used for the call |
| `model` | Model name used |
| `input_tokens` | Token count from provider response |
| `output_tokens` | Token count from provider response |
| `estimated_cost` | Calculated from token counts and pricing table |
| `latency_ms` | Wall-clock time for the provider call |
| `status` | `"success"`, `"error"`, or `"timeout"` |
| `error_message` | Error details if status is not `"success"` |
| `request_metadata` | JSON with template_slug, variable keys (not values), model config |

**Important:** Never log the full prompt text or response text in `request_metadata`. Log only structural metadata (template slug, variable key names, model parameters) to avoid storing sensitive user content in the audit trail.

## Token Counting

Each provider adapter implements `countTokens()` to estimate token usage before sending a request. This is used for:

1. **Pre-flight checks** -- reject requests that would exceed the model's context window
2. **Cost estimation** -- provide estimated cost before execution (for future use)
3. **Post-response verification** -- compare estimated vs. actual token counts from the provider response

Use the provider's reported token counts (from the API response) for audit logging, not the local estimate. The local estimate is a safeguard only.

## Cost Estimation

The gateway calculates `estimated_cost` using per-model pricing stored in a configuration map. See `references/llm-pricing-table.md` for current rates.

```typescript
function estimateCost(provider: string, model: string, inputTokens: number, outputTokens: number): number {
  const pricing = MODEL_PRICING[provider]?.[model];
  if (!pricing) return 0;
  return (inputTokens / 1_000_000) * pricing.input + (outputTokens / 1_000_000) * pricing.output;
}
```

Store the pricing map in a configuration file (`lib/llm/pricing.ts`) so it can be updated without code changes to the gateway logic.

## Model Configuration

Each request can specify model parameters. Defaults apply when not provided:

| Parameter | Default | Range | Description |
|---|---|---|---|
| `temperature` | 0.7 | 0 -- 2 | Randomness of output |
| `max_tokens` | 1024 | 1 -- 128000 | Maximum response length |
| `stop_sequences` | `[]` | -- | Sequences that stop generation |

When using a template, the template's stored `temperature` and `max_tokens` are used unless the request explicitly overrides them.

## Error Handling

The gateway catches all provider errors and returns structured error responses. It never lets raw SDK errors propagate to the client.

| Error Type | HTTP Status | Logged Status | Behavior |
|---|---|---|---|
| Provider API error | 502 | `"error"` | Log error, return generic message |
| Rate limit hit | 429 | `"error"` | Log error, return retry-after if available |
| Request timeout (30s) | 504 | `"timeout"` | Log timeout, return timeout message |
| Invalid request | 400 | not logged | Return validation errors |
| Unauthorized | 401 / 403 | not logged | Return auth error |
| Template not found | 404 | not logged | Return not-found error |

Provider errors are logged to `llm_audit_log` with `status = 'error'` and the error message captured. The error message returned to the client should be generic ("LLM provider returned an error") while the full error is stored in the audit log for admin review.

### Timeout Handling

Set a 30-second timeout on all provider calls. If the provider does not respond within 30 seconds, abort the request, log a timeout entry, and return a 504 to the client. For streaming requests, if no chunks arrive within 30 seconds, close the stream and log the timeout.

## File Structure

```
lib/llm/
  gateway.ts              -- Main gateway service (orchestrates everything)
  providers/
    index.ts              -- Provider registry and factory
    {{#each providers_display}}
    {{this.id}}.ts{{#unless @last}}              {{/unless}}
    {{/each}}
  pricing.ts              -- Per-model pricing configuration
  types.ts                -- Shared TypeScript interfaces

app/api/llm/chat/
  route.ts                -- POST handler (calls gateway service)
```

## Implementation Checklist

1. Create the `LLMProvider` interface and types in `lib/llm/types.ts`
2. Implement provider adapters ({{#each providers_display}}`{{this.id}}.ts`{{#unless @last}}, {{/unless}}{{/each}})
3. Create the provider registry in `lib/llm/providers/index.ts`
4. Build the pricing configuration in `lib/llm/pricing.ts`
5. Implement the gateway service in `lib/llm/gateway.ts`
6. Create the `POST /api/llm/chat` route handler
7. Add authorization middleware (role check for {{llm_access_roles_joined}})
8. Add template resolution and variable interpolation
9. Add streaming support with SSE
10. Add audit logging (write to `llm_audit_log` via service role client)
11. Add error handling and timeout logic
12. Test with both template-based and ad-hoc calls
