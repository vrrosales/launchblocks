# Module 5: LLM Gateway

> Spec for **{{app_name}}** -- generated by Launchblocks

## Purpose

The LLM Gateway is the single entry point for **every** LLM call in the application. It is implemented as a **two-service architecture**:

1. **Python Microservice** (FastAPI + Celeste AI SDK) — handles all LLM provider calls
2. **Next.js API Route** — handles auth, template resolution, variable interpolation, proxying to Python, and audit logging

No feature code, page, component, or server action should ever call an LLM provider directly. All LLM interactions flow through the Next.js `/api/llm/chat` route, which delegates the actual LLM call to the Python microservice.

This architecture ensures that switching providers, adding rate limits, or changing logging behavior requires changes in exactly one module rather than scattered across the codebase.

## Architecture Overview

```
Feature Code (pages, components, server actions)
        |
        v
  POST /api/llm/chat             <-- Next.js route (Vercel)
   ├── Authorization check (role-based)
   ├── Template resolution (Supabase DB)
   ├── Variable interpolation
   ├── Forward resolved prompt ──────────→  Python Microservice
   │                                         ├── FastAPI
   │                                         ├── Celeste AI SDK
   │                                         ├── POST /generate
   │                                         ├── POST /stream
   │                                         └── GET /health
   ├── Receive response  ←──────────────────┘
   ├── Cost estimation
   ├── Audit logging (writes to llm_audit_log)
   └── Return to client
```

**Rule:** Import the gateway client utility in feature code, never call the Python service or a provider SDK directly.

## Supported Providers

The Python microservice uses the Celeste AI SDK, which provides a unified interface to all configured providers:

{{#each providers_display}}
- **{{this.name}}** (`{{this.id}}`) — env var: `{{this.env_var}}`
{{/each}}

{{#if has_multiple_providers}}
The gateway selects the correct provider based on the `provider` field in the request or the provider configured on the prompt template. If no provider is specified, the first configured provider (`{{providers_display.[0].id}}`) is used as the default.
{{else}}
This project uses a single provider (`{{providers_display.[0].id}}`). The Celeste SDK abstraction is still in place so additional providers can be added later without changing feature code.
{{/if}}

## Python Microservice

### Setup

Create a `services/llm/` directory at the project root with the following files:

```
services/llm/
  main.py                # FastAPI application
  requirements.txt       # Python dependencies
  .env                   # Provider API keys (not committed)
```

**requirements.txt:**
```
celeste-ai>=0.10,<1.0
fastapi>=0.110,<1.0
uvicorn>=0.29,<1.0
```

> **Celeste SDK version note:** Pin `celeste-ai>=0.10,<1.0`. The Celeste AI SDK API may evolve — always consult the official Celeste documentation for current usage patterns rather than hardcoding SDK method names.

### Endpoints

#### `POST /generate`

Non-streaming LLM call. Accepts a JSON body and returns the complete response.

**Request:**
```json
{
  "provider": "{{providers_display.[0].id}}",
  "model": "gpt-4o",
  "messages": [
    { "role": "system", "content": "You are a helpful assistant." },
    { "role": "user", "content": "Explain quantum computing." }
  ],
  "temperature": 0.7,
  "max_tokens": 1024
}
```

**Response:**
```json
{
  "content": "Quantum computing is...",
  "model": "gpt-4o",
  "provider": "{{providers_display.[0].id}}",
  "usage": {
    "input_tokens": 42,
    "output_tokens": 287
  },
  "latency_ms": 1823
}
```

#### `POST /stream`

Streaming LLM call. Same request body as `/generate`, returns `text/event-stream` (SSE):

```
data: {"content": "Quantum ", "done": false}
data: {"content": "computing ", "done": false}
data: {"content": "is...", "done": false}
data: {"content": "", "done": true, "usage": {"input_tokens": 42, "output_tokens": 287}, "latency_ms": 1823}
```

The final chunk (`done: true`) includes the full usage summary.

#### `GET /health`

Returns `{ "status": "ok" }` with HTTP 200. Used for service health checks and deployment verification.

### Environment Variables

The Python service needs its own `.env` file with provider API keys:

```env
{{#each providers_display}}
{{this.env_var}}={{this.env_placeholder}}
{{/each}}
```

These are the same API keys listed in the Next.js `.env.local`, but the Python service reads them independently. In production, configure these as environment variables on the hosting platform (Railway, Fly.io, Cloud Run, etc.).

### Error Handling

The Python service should:
- Return structured JSON errors: `{ "error": "message", "code": "ERROR_CODE" }`
- Catch all Celeste SDK / provider errors and return 502 with a generic message
- Implement a 30-second timeout on all LLM calls
- Return 504 on timeout
- Never expose raw SDK error details to the caller

## Next.js API Route

### `POST /api/llm/chat`

This is the only endpoint feature code should call for LLM interactions. It orchestrates auth, template resolution, proxying, and audit logging.

#### Request Body -- Template-Based Call

Use a template-based call when referencing a saved prompt template by its slug. The Next.js route resolves the template from Supabase, interpolates variables, and forwards the assembled messages to the Python service.

```json
{
  "template_slug": "summarize-article",
  "variables": {
    "article_text": "The full article content here...",
    "max_length": "3 sentences"
  },
  "stream": false
}
```

#### Request Body -- Ad-Hoc Call

Use an ad-hoc call when feature code needs to send raw prompts without a saved template. The caller specifies the messages, model, and provider directly.

```json
{
  "messages": [
    { "role": "system", "content": "You are a helpful assistant." },
    { "role": "user", "content": "Explain quantum computing in simple terms." }
  ],
  "provider": "{{providers_display.[0].id}}",
  "model": "gpt-4o",
  "temperature": 0.7,
  "max_tokens": 1024,
  "stream": false
}
```

#### Success Response (non-streaming)

```json
{
  "content": "The generated response text...",
  "model": "gpt-4o",
  "provider": "{{providers_display.[0].id}}",
  "usage": {
    "input_tokens": 142,
    "output_tokens": 287,
    "total_tokens": 429
  },
  "estimated_cost": 0.003215,
  "latency_ms": 1823,
  "audit_id": "uuid-of-the-audit-log-entry"
}
```

#### Streaming Response

When `"stream": true`, the Next.js route proxies the SSE stream from the Python service to the client:

```
data: {"content": "The ", "done": false}
data: {"content": "generated ", "done": false}
data: {"content": "response.", "done": false}
data: {"content": "", "done": true, "usage": {"input_tokens": 142, "output_tokens": 12, "total_tokens": 154}, "estimated_cost": 0.001, "latency_ms": 890, "audit_id": "uuid"}
```

The final chunk includes `estimated_cost` and `audit_id` (added by Next.js after receiving the Python service's final chunk).

### Route Handler Flow

```
1. Parse and validate request body (Zod)
2. Authenticate user via Supabase session
3. Check user status === 'approved'
4. Check user role is in LLM access list
5. If template_slug provided:
   a. Load template from prompt_templates (Supabase)
   b. Check is_active === true
   c. Interpolate variables into system_prompt and user_prompt_template
   d. Build messages array
6. Forward request to Python service (POST /generate or POST /stream)
7. Receive response from Python service
8. Calculate estimated_cost from usage data
9. Write audit log entry to llm_audit_log (Supabase service role)
10. Return response to client
```

## Authorization

Only users whose role is in the LLM access list can call the gateway.

**Authorized roles:** {{llm_access_roles_joined}}

{{#if all_roles_have_llm}}
All roles in this project have LLM access, so every authenticated and approved user can make calls.
{{else}}
Not all roles have LLM access. The gateway must check the user's role before processing any request.
{{/if}}

Implementation:

```typescript
// In the Next.js API route handler
const user = await getAuthenticatedUser(request);
if (!user || user.status !== "approved") {
  return Response.json({ error: "Unauthorized" }, { status: 401 });
}

const allowedRoles = [{{#each llm_access_roles}}"{{this}}"{{#unless @last}}, {{/unless}}{{/each}}];
if (!allowedRoles.includes(user.role)) {
  return Response.json({ error: "Your role does not have LLM access" }, { status: 403 });
}
```

## Template Resolution

When a request includes `template_slug`, the Next.js route:

1. Queries `prompt_templates` by slug where `is_active = true`
2. Returns 404 if the template does not exist or is inactive
3. Reads the template's `system_prompt`, `user_prompt_template`, `provider`, `model`, `temperature`, and `max_tokens`
4. Interpolates `\{{variable_name}}` placeholders in both prompts using the provided `variables` object
5. Assembles the messages array: system message from `system_prompt`, user message from the interpolated `user_prompt_template`
6. Forwards the assembled request to the Python microservice

Variable interpolation uses simple string replacement. Any `\{{key}}` in the template that does not have a matching key in `variables` should be left as-is (do not throw an error, as some templates may include optional variables).

## Audit Logging

Every call through the gateway -- successful or failed -- is logged to the `llm_audit_log` table. The **Next.js route** writes the audit entry **after** receiving the response from the Python service (or after an error/timeout), using the Supabase service role client to bypass RLS.

Fields logged per call:

| Field | Source |
|---|---|
| `user_id` | Authenticated user's ID |
| `prompt_template_id` | UUID of the template (null for ad-hoc calls) |
| `provider` | Provider ID used for the call |
| `model` | Model name used |
| `input_tokens` | Token count from Python service response |
| `output_tokens` | Token count from Python service response |
| `estimated_cost` | Calculated from token counts and pricing table |
| `latency_ms` | Wall-clock time from request start to response complete |
| `status` | `"success"`, `"error"`, or `"timeout"` |
| `error_message` | Error details if status is not `"success"` |
| `request_metadata` | JSON with template_slug, variable keys (not values), model config |

**Important:** Never log the full prompt text or response text in `request_metadata`. Log only structural metadata (template slug, variable key names, model parameters) to avoid storing sensitive user content in the audit trail.

## Streaming

Streaming uses an SSE proxy chain:

```
Celeste SDK → FastAPI SSE (/stream) → Next.js SSE (/api/llm/chat) → Client
```

1. Client sends `POST /api/llm/chat` with `"stream": true`
2. Next.js route does auth + template resolution, then calls Python `POST /stream`
3. Python service streams chunks from Celeste back as SSE events
4. Next.js proxies each chunk to the client as-is
5. On the final chunk (`done: true`), Next.js reads usage data, calculates cost, writes audit log
6. Next.js appends `estimated_cost` and `audit_id` to the final chunk before forwarding

## Cost Estimation

The Next.js route calculates `estimated_cost` using per-model pricing stored in a configuration map. See `references/llm-pricing-table.md` for current rates.

```typescript
function estimateCost(provider: string, model: string, inputTokens: number, outputTokens: number): number {
  const pricing = MODEL_PRICING[provider]?.[model];
  if (!pricing) return 0;
  return (inputTokens / 1_000_000) * pricing.input + (outputTokens / 1_000_000) * pricing.output;
}
```

Store the pricing map in a configuration file (`lib/llm/pricing.ts`) so it can be updated without code changes.

## Error Handling

The gateway catches all errors and returns structured error responses. It never lets raw errors propagate to the client.

| Error Type | HTTP Status | Logged Status | Behavior |
|---|---|---|---|
| Python service error (LLM) | 502 | `"error"` | Log error, return generic message |
| Rate limit hit | 429 | `"error"` | Log error, return retry-after if available |
| Request timeout (30s) | 504 | `"timeout"` | Log timeout, return timeout message |
| Python service unreachable | 503 | `"error"` | Log error, return "LLM service unavailable" |
| Invalid request | 400 | not logged | Return validation errors |
| Unauthorized | 401 / 403 | not logged | Return auth error |
| Template not found | 404 | not logged | Return not-found error |

Provider errors are logged to `llm_audit_log` with `status = 'error'` and the error message captured. The error message returned to the client should be generic ("LLM provider returned an error") while the full error is stored in the audit log for admin review.

### Timeout Handling

Set a 30-second timeout on the HTTP call to the Python service. If the Python service does not respond within 30 seconds, abort the request, log a timeout entry, and return a 504 to the client. For streaming requests, if no chunks arrive within 30 seconds, close the stream and log the timeout.

## File Structure

```
services/llm/
  main.py                          -- FastAPI app (Celeste AI SDK)
  requirements.txt                 -- celeste-ai, fastapi, uvicorn
  .env                             -- Provider API keys (not committed)

src/lib/llm/
  service-client.ts                -- HTTP client for the Python microservice
  cost-calculator.ts               -- Token cost estimation (pricing map)
  types.ts                         -- Shared TypeScript interfaces

src/app/api/llm/chat/
  route.ts                         -- POST handler (auth → template → proxy → audit)
```

## Implementation Checklist

1. Create `services/llm/requirements.txt` with `celeste-ai`, `fastapi`, `uvicorn`
2. Implement `services/llm/main.py` with `/generate`, `/stream`, and `/health` endpoints
3. Configure Celeste SDK with provider API keys from environment variables
4. Test the Python service standalone: `uvicorn main:app --reload`
5. Create TypeScript types in `src/lib/llm/types.ts`
6. Build the service client in `src/lib/llm/service-client.ts` (HTTP calls to Python)
7. Build the pricing configuration in `src/lib/llm/cost-calculator.ts`
8. Create the `POST /api/llm/chat` route handler in Next.js
9. Add authorization middleware (role check for {{llm_access_roles_joined}})
10. Add template resolution and variable interpolation (Supabase DB query)
11. Add streaming support with SSE proxy chain
12. Add audit logging (write to `llm_audit_log` via service role client)
13. Add error handling and timeout logic (30s)
14. Set `LLM_SERVICE_URL` environment variable in `.env.local` and Vercel
15. Test with both template-based and ad-hoc calls, streaming and non-streaming
