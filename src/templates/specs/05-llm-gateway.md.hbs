# Module 5: LLM Gateway

> Spec for **{{app_name}}** -- generated by Launchblocks

## Purpose

The LLM Gateway is the single entry point for **every** LLM call in the application. It is implemented as a **two-service architecture**:

1. **Python Microservice** (FastAPI + Celeste AI SDK) — handles all LLM provider calls
2. **Next.js API Route** — handles auth, template resolution, variable interpolation, proxying to Python, and audit logging

No feature code, page, component, or server action should ever call an LLM provider directly. All LLM interactions flow through the Next.js `/api/llm/chat` route, which delegates the actual LLM call to the Python microservice.

This architecture ensures that switching providers, adding rate limits, or changing logging behavior requires changes in exactly one module rather than scattered across the codebase.

## Architecture Overview

```
Feature Code (pages, components, server actions)
        |
        v
  POST /api/llm/chat             <-- Next.js route (Vercel)
   ├── Authorization check (role-based)
   ├── Template resolution (Supabase DB)
   ├── Variable interpolation
   ├── Forward resolved prompt ──────────→  Python Microservice
   │                                         ├── FastAPI
   │                                         ├── Celeste AI SDK
   │                                         ├── POST /generate
   │                                         ├── POST /stream
   │                                         └── GET /health
   ├── Receive response  ←──────────────────┘
   ├── Cost estimation
   ├── Audit logging (writes to llm_audit_log)
   └── Return to client
```

**Rule:** Import the gateway client utility in feature code, never call the Python service or a provider SDK directly.

## Supported Providers

The Python microservice uses the Celeste AI SDK, which provides a unified interface to all configured providers:

{{#each providers_display}}
- **{{this.name}}** (`{{this.id}}`) — env var: `{{this.env_var}}`
{{/each}}

{{#if has_multiple_providers}}
The gateway selects the correct provider based on the `provider` field in the request or the provider configured on the prompt template. If no provider is specified, the first configured provider (`{{providers_display.[0].id}}`) is used as the default.
{{else}}
This project uses a single provider (`{{providers_display.[0].id}}`). The Celeste SDK abstraction is still in place so additional providers can be added later without changing feature code.
{{/if}}

## Python Microservice

### Setup

Create a `services/llm/` directory at the project root with the following files:

```
services/llm/
  main.py                # FastAPI application
  requirements.txt       # Python dependencies
  .env                   # Provider API keys (not committed)
```

**requirements.txt:**
```
celeste-ai>=0.10,<1.0
fastapi>=0.110,<1.0
uvicorn>=0.29,<1.0
```

> **Celeste SDK version note:** Pin `celeste-ai>=0.10,<1.0`. The Celeste AI SDK API may evolve — always consult the official Celeste documentation for current usage patterns rather than hardcoding SDK method names.

### Endpoints

#### `POST /generate`

Non-streaming LLM call. Accepts a JSON body and returns the complete response.

**Request:**
```json
{
  "provider": "{{providers_display.[0].id}}",
  "model": "gpt-4o",
  "messages": [
    { "role": "system", "content": "You are a helpful assistant." },
    { "role": "user", "content": "Explain quantum computing." }
  ],
  "temperature": 0.7,
  "max_tokens": 1024
}
```

**Response:**
```json
{
  "content": "Quantum computing is...",
  "model": "gpt-4o",
  "provider": "{{providers_display.[0].id}}",
  "usage": {
    "input_tokens": 42,
    "output_tokens": 287
  },
  "latency_ms": 1823
}
```

#### `POST /stream`

Streaming LLM call. Same request body as `/generate`, returns `text/event-stream` (SSE):

```
data: {"content": "Quantum ", "done": false}
data: {"content": "computing ", "done": false}
data: {"content": "is...", "done": false}
data: {"content": "", "done": true, "usage": {"input_tokens": 42, "output_tokens": 287}, "latency_ms": 1823}
```

The final chunk (`done: true`) includes the full usage summary.

#### `GET /health`

Returns `{ "status": "ok" }` with HTTP 200. Used for service health checks and deployment verification.

### Environment Variables

The Python service needs its own `.env` file with provider API keys:

```env
{{#each providers_display}}
{{this.env_var}}={{this.env_placeholder}}
{{/each}}
```

These are the same API keys listed in the Next.js `.env.local`, but the Python service reads them independently. In production, configure these as environment variables on the hosting platform (Railway, Fly.io, Cloud Run, etc.).

### Error Handling

The Python service should:
- Return structured JSON errors: `{ "error": "message", "code": "ERROR_CODE" }`
- Catch all Celeste SDK / provider errors and return 502 with a generic message
- Implement a 30-second timeout on all LLM calls
- Return 504 on timeout
- Never expose raw SDK error details to the caller

## Next.js API Route

### `POST /api/llm/chat`

This is the only endpoint feature code should call for LLM interactions. It orchestrates auth, template resolution, proxying, and audit logging.

#### Request Body -- Template-Based Call

Use a template-based call when referencing a saved prompt template by its slug. The Next.js route resolves the template from Supabase, interpolates variables, and forwards the assembled messages to the Python service.

```json
{
  "template_slug": "summarize-article",
  "variables": {
    "article_text": "The full article content here...",
    "max_length": "3 sentences"
  },
  "stream": false
}
```

#### Request Body -- Ad-Hoc Call

Use an ad-hoc call when feature code needs to send raw prompts without a saved template. The caller specifies the messages, model, and provider directly.

```json
{
  "messages": [
    { "role": "system", "content": "You are a helpful assistant." },
    { "role": "user", "content": "Explain quantum computing in simple terms." }
  ],
  "provider": "{{providers_display.[0].id}}",
  "model": "gpt-4o",
  "temperature": 0.7,
  "max_tokens": 1024,
  "stream": false
}
```

#### Success Response (non-streaming)

```json
{
  "content": "The generated response text...",
  "model": "gpt-4o",
  "provider": "{{providers_display.[0].id}}",
  "usage": {
    "input_tokens": 142,
    "output_tokens": 287,
    "total_tokens": 429
  },
  "estimated_cost": 0.003215,
  "latency_ms": 1823,
  "audit_id": "uuid-of-the-audit-log-entry"
}
```

#### Streaming Response

When `"stream": true`, the Next.js route proxies the SSE stream from the Python service to the client:

```
data: {"content": "The ", "done": false}
data: {"content": "generated ", "done": false}
data: {"content": "response.", "done": false}
data: {"content": "", "done": true, "usage": {"input_tokens": 142, "output_tokens": 12, "total_tokens": 154}, "estimated_cost": 0.001, "latency_ms": 890, "audit_id": "uuid"}
```

The final chunk includes `estimated_cost` and `audit_id` (added by Next.js after receiving the Python service's final chunk).

### Route Handler Flow

```
1. Parse and validate request body (Zod)
2. Authenticate user via Supabase session
3. Check user status === 'approved'
4. Check user role is in LLM access list
5. If template_slug provided:
   a. Load template from prompt_templates (Supabase)
   b. Check is_active === true
   c. Interpolate variables into system_prompt and user_prompt_template
   d. Build messages array
6. Forward request to Python service (POST /generate or POST /stream)
7. Receive response from Python service
8. Calculate estimated_cost from usage data
9. Write audit log entry to llm_audit_log (Supabase service role)
10. Return response to client
```

## Authorization

Only users whose role is in the LLM access list can call the gateway.

**Authorized roles:** {{llm_access_roles_joined}}

{{#if all_roles_have_llm}}
All roles in this project have LLM access, so every authenticated and approved user can make calls.
{{else}}
Not all roles have LLM access. The gateway must check the user's role before processing any request.
{{/if}}

Implementation:

```typescript
// In the Next.js API route handler
const user = await getAuthenticatedUser(request);
if (!user || user.status !== "approved") {
  return Response.json({ error: "Unauthorized" }, { status: 401 });
}

const allowedRoles = [{{#each llm_access_roles}}"{{this}}"{{#unless @last}}, {{/unless}}{{/each}}];
if (!allowedRoles.includes(user.role)) {
  return Response.json({ error: "Your role does not have LLM access" }, { status: 403 });
}
```

## Template Resolution

When a request includes `template_slug`, the Next.js route:

1. Queries `prompt_templates` by slug where `is_active = true`
2. Returns 404 if the template does not exist or is inactive
3. Reads the template's `system_prompt`, `user_prompt_template`, `provider`, `model`, `temperature`, and `max_tokens`
4. Interpolates `\{{variable_name}}` placeholders in both prompts using the provided `variables` object
5. Assembles the messages array: system message from `system_prompt`, user message from the interpolated `user_prompt_template`
6. Forwards the assembled request to the Python microservice

Variable interpolation uses simple string replacement. Any `\{{key}}` in the template that does not have a matching key in `variables` should be left as-is (do not throw an error, as some templates may include optional variables).

## Audit Logging

Every call through the gateway -- successful or failed -- is logged to the `llm_audit_log` table. The **Next.js route** writes the audit entry **after** receiving the response from the Python service (or after an error/timeout), using the Supabase service role client to bypass RLS.

Fields logged per call:

| Field | Source |
|---|---|
| `user_id` | Authenticated user's ID |
| `prompt_template_id` | UUID of the template (null for ad-hoc calls) |
| `provider` | Provider ID used for the call |
| `model` | Model name used |
| `input_tokens` | Token count from Python service response |
| `output_tokens` | Token count from Python service response |
| `estimated_cost` | Calculated from token counts and pricing table |
| `latency_ms` | Wall-clock time from request start to response complete |
| `status` | `"success"`, `"error"`, or `"timeout"` |
| `error_message` | Error details if status is not `"success"` |
| `request_metadata` | JSON with template_slug, variable keys (not values), model config |

**Important:** Never log the full prompt text or response text in `request_metadata`. Log only structural metadata (template slug, variable key names, model parameters) to avoid storing sensitive user content in the audit trail.

## Streaming

Streaming uses an SSE proxy chain:

```
Celeste SDK → FastAPI SSE (/stream) → Next.js SSE (/api/llm/chat) → Client
```

1. Client sends `POST /api/llm/chat` with `"stream": true`
2. Next.js route does auth + template resolution, then calls Python `POST /stream`
3. Python service streams chunks from Celeste back as SSE events
4. Next.js proxies each chunk to the client as-is
5. On the final chunk (`done: true`), Next.js reads usage data, calculates cost, writes audit log
6. Next.js appends `estimated_cost` and `audit_id` to the final chunk before forwarding

## Cost Estimation

The Next.js route calculates `estimated_cost` using per-model pricing stored in a configuration map. See `references/llm-pricing-table.md` for current rates.

```typescript
function estimateCost(provider: string, model: string, inputTokens: number, outputTokens: number): number {
  const pricing = MODEL_PRICING[provider]?.[model];
  if (!pricing) return 0;
  return (inputTokens / 1_000_000) * pricing.input + (outputTokens / 1_000_000) * pricing.output;
}
```

Store the pricing map in a configuration file (`lib/llm/pricing.ts`) so it can be updated without code changes.

## Error Handling

The gateway catches all errors and returns structured error responses. It never lets raw errors propagate to the client.

| Error Type | HTTP Status | Logged Status | Behavior |
|---|---|---|---|
| Python service error (LLM) | 502 | `"error"` | Log error, return generic message |
| Rate limit hit | 429 | `"error"` | Log error, return retry-after if available |
| Request timeout (30s) | 504 | `"timeout"` | Log timeout, return timeout message |
| Python service unreachable | 503 | `"error"` | Log error, return "LLM service unavailable" |
| Invalid request | 400 | not logged | Return validation errors |
| Unauthorized | 401 / 403 | not logged | Return auth error |
| Template not found | 404 | not logged | Return not-found error |

Provider errors are logged to `llm_audit_log` with `status = 'error'` and the error message captured. The error message returned to the client should be generic ("LLM provider returned an error") while the full error is stored in the audit log for admin review.

### Timeout Handling

Set a 30-second timeout on the HTTP call to the Python service. If the Python service does not respond within 30 seconds, abort the request, log a timeout entry, and return a 504 to the client. For streaming requests, if no chunks arrive within 30 seconds, close the stream and log the timeout.

## File Structure

```
services/llm/
  main.py                          -- FastAPI app (Celeste AI SDK)
  requirements.txt                 -- celeste-ai, fastapi, uvicorn
  .env                             -- Provider API keys (not committed)

src/lib/llm/
  service-client.ts                -- HTTP client for the Python microservice
  cost-calculator.ts               -- Token cost estimation (pricing map)
  types.ts                         -- Shared TypeScript interfaces

src/app/api/llm/chat/
  route.ts                         -- POST handler (auth → template → proxy → audit)
```

## Implementation Tasks

Complete these tasks in order. Each task builds on the previous ones.

### Task 5.1: Set Up Python Microservice
Create `services/llm/` with `requirements.txt` (`celeste-ai`, `fastapi`, `uvicorn`), `main.py` (empty FastAPI app), and `.env`. Install dependencies with `pip install -r requirements.txt`.
**Verify:** Run `uvicorn main:app --reload` and confirm the server starts without errors.

### Task 5.2: Implement GET /health Endpoint
Add a health check endpoint to the Python service that returns `{ "status": "ok" }`.
**Verify:** `curl http://localhost:8000/health` returns `{"status": "ok"}` with HTTP 200.

### Task 5.3: Implement POST /generate Endpoint
Build the non-streaming LLM call endpoint. Accept `{ provider, model, messages, temperature, max_tokens }`, call the Celeste AI SDK, and return `{ content, usage, latency_ms }`. Add 30-second timeout and error handling.
**Verify:** Send a test request with a simple prompt and confirm a valid LLM response is returned with usage data.

### Task 5.4: Implement POST /stream Endpoint
Build the streaming LLM call endpoint. Accept the same request body as `/generate`, return SSE (`text/event-stream`) with content chunks. The final chunk includes `done: true` with usage data.
**Verify:** Send a streaming request and confirm content arrives as SSE chunks, ending with a `done: true` event.

### Task 5.5: Create TypeScript Types and Service Client
Create `src/lib/llm/types.ts` with shared interfaces (LLM request/response shapes) and `src/lib/llm/service-client.ts` with an HTTP client that calls the Python microservice endpoints.
**Verify:** Import the service client and confirm TypeScript compiles without errors.

### Task 5.6: Build Cost Calculator
Create `src/lib/llm/cost-calculator.ts` with per-model pricing data and an `estimateCost(provider, model, inputTokens, outputTokens)` function. Reference `references/llm-pricing-table.md` for rates.
**Verify:** Call `estimateCost("{{providers_display.[0].id}}", "gpt-4o", 1000, 500)` and confirm it returns a reasonable cost estimate.

### Task 5.7: Create POST /api/llm/chat Route Handler
Build the Next.js API route that orchestrates the full LLM call flow: parse request, authenticate user, check status, check role access, forward to Python service, receive response.
**Verify:** Send a valid ad-hoc request to `/api/llm/chat` as an authenticated user and confirm an LLM response is returned.

### Task 5.8: Add Authorization Middleware
Implement role-based access control in the route handler. Only users with roles in the LLM access list ({{llm_access_roles_joined}}) can make calls. Return 403 for unauthorized roles.
{{#if all_roles_have_llm}}
**Verify:** Confirm all authenticated, approved users can make LLM calls.
{{else}}
**Verify:** Confirm users with unauthorized roles receive a 403 error when calling the gateway.
{{/if}}

### Task 5.9: Implement Template Resolution
Add template-based call support. When `template_slug` is provided, load the template from `prompt_templates`, validate it is active, interpolate variables, and build the messages array.
**Verify:** Create a prompt template, send a request with its slug and variables, and confirm the resolved prompt reaches the LLM.

### Task 5.10: Add Streaming Proxy Support
Implement SSE proxy in the Next.js route for streaming calls. Forward chunks from the Python service to the client, appending `estimated_cost` and `audit_id` to the final chunk.
**Verify:** Send a streaming request through the Next.js route and confirm chunks arrive at the client in real-time.

### Task 5.11: Add Audit Logging
After every LLM call (success or failure), write an entry to `llm_audit_log` using the Supabase service role client. Log user_id, provider, model, tokens, cost, latency, and status.
**Verify:** Make an LLM call and confirm a corresponding row appears in `llm_audit_log` with correct data.

### Task 5.12: Configure Environment and Deploy
Set `LLM_SERVICE_URL` in `.env.local` (for local dev) and in the Vercel dashboard (for production). Deploy the Python service to a Python-friendly platform.
**Verify:** The Next.js route successfully proxies requests to the deployed Python service.

---

## Test Specifications

### Unit Tests
- [ ] `estimateCost()` returns correct cost for known model pricing
- [ ] `estimateCost()` returns 0 for unknown models
- [ ] Variable interpolation replaces `\{{variable}}` with provided values
- [ ] Variable interpolation leaves unmatched `\{{variable}}` placeholders as-is
- [ ] Request body validation rejects missing required fields
- [ ] Request body validation accepts both template-based and ad-hoc call formats
- [ ] Authorization check returns 403 for roles not in the LLM access list
- [ ] Authorization check returns 401 for unauthenticated requests

### Integration Tests
- [ ] Python service `/generate` endpoint returns valid LLM response with usage data
- [ ] Python service `/stream` endpoint returns SSE chunks with final usage summary
- [ ] Python service returns structured error on provider failure (502)
- [ ] Python service returns 504 on timeout
- [ ] Next.js route resolves template by slug from the database
- [ ] Next.js route returns 404 for inactive or non-existent templates
- [ ] Next.js route writes audit entry on successful call
- [ ] Next.js route writes audit entry with `status = 'error'` on LLM failure
- [ ] Next.js route writes audit entry with `status = 'timeout'` on timeout
- [ ] Streaming proxy forwards chunks correctly from Python service to client
{{#if has_multiple_providers}}
- [ ] Gateway correctly routes requests to different providers based on template configuration
{{/if}}

### E2E Tests
- [ ] Template-based call: create template → send request with slug and variables → receive response → audit entry logged
- [ ] Ad-hoc call: send raw messages → receive response → audit entry logged
- [ ] Streaming call: send streaming request → receive SSE chunks → final chunk has usage data and audit_id
- [ ] Error handling: send request with invalid API key → receive structured error → error logged in audit
- [ ] Rate limit: verify 429 is returned and logged when provider rate-limits the request
