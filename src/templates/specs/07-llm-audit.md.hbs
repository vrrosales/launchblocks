# Module 7: LLM Audit Trail

> Spec for **{{app_name}}** -- generated by Launchblocks

## Purpose

The LLM Audit Trail provides a complete, tamper-resistant record of every LLM call made through the gateway. It powers an admin dashboard for monitoring usage, tracking costs, reviewing errors, and exporting data. Every request logged by the LLM Gateway (Module 5) is surfaced here with filtering, aggregation, and visualization.

## Authorization

| Action | Required Permission | Roles |
|---|---|---|
| View audit dashboard | `view_audit_log` | {{#each role_permission_summary}}{{#if has_permissions}}{{this.name}}{{#unless @last}}, {{/unless}}{{/if}}{{/each}} |
| Export audit data to CSV | `export_audit_log` | {{#each role_permission_summary}}{{#if has_permissions}}{{this.name}}{{#unless @last}}, {{/unless}}{{/if}}{{/each}} |
| View own usage | (any authenticated user) | {{role_names_joined}} |

Note: Individual users can always see their own audit entries (the `audit_select_own` RLS policy). The admin dashboard and export features require explicit permissions.

## Database Schema

The audit trail reads from two sources created in migration `004_llm_audit_log`:

### `llm_audit_log` Table

| Column | Type | Description |
|---|---|---|
| `id` | uuid | Primary key |
| `user_id` | uuid | FK to `user_profiles.id` -- who made the call |
| `prompt_template_id` | uuid | FK to `prompt_templates.id` -- null for ad-hoc calls |
| `provider` | text | Provider used ({{#each providers_display}}`{{this.id}}`{{#unless @last}}, {{/unless}}{{/each}}) |
| `model` | text | Model name used |
| `input_tokens` | integer | Tokens in the request |
| `output_tokens` | integer | Tokens in the response |
| `total_tokens` | integer | Generated column: `input_tokens + output_tokens` |
| `estimated_cost` | numeric(10,6) | Estimated USD cost of the call |
| `latency_ms` | integer | Round-trip time in milliseconds |
| `status` | text | `"success"`, `"error"`, or `"timeout"` |
| `error_message` | text | Error details (null on success) |
| `request_metadata` | jsonb | Structural metadata (template slug, model config, etc.) |
| `created_at` | timestamptz | When the call was made |

### `llm_audit_summary` View

Pre-aggregated data for dashboard charts, grouped by day, provider, and model:

| Column | Type | Description |
|---|---|---|
| `day` | timestamptz | Truncated to day |
| `provider` | text | Provider name |
| `model` | text | Model name |
| `request_count` | bigint | Number of calls |
| `total_input_tokens` | bigint | Sum of input tokens |
| `total_output_tokens` | bigint | Sum of output tokens |
| `total_tokens` | bigint | Sum of all tokens |
| `total_cost` | numeric | Sum of estimated costs |
| `avg_latency_ms` | integer | Average response time |
| `error_count` | bigint | Number of failed calls |

Use this view for dashboard charts instead of querying the raw log table -- it is significantly faster for time-series data.

## Dashboard Metrics

The audit dashboard displays the following key metrics, each derived from the data sources above.

### Summary Cards (Top of Dashboard)

Display these as prominent metric cards at the top of the page:

| Metric | Calculation | Description |
|---|---|---|
| Total Calls | `COUNT(*)` from `llm_audit_log` | Total number of LLM calls |
| Total Tokens | `SUM(total_tokens)` | Combined input + output tokens |
| Total Cost | `SUM(estimated_cost)` | Estimated USD spent |
| Error Rate | `COUNT(*) WHERE status != 'success' / COUNT(*)` | Percentage of failed calls |
| Avg Latency | `AVG(latency_ms)` | Average response time |

Each card should show the value for the selected date range and a comparison to the previous period (e.g., "+12% vs. last 7 days").

### Breakdown Tables

Below the summary cards, show breakdowns:

**By Provider:**
| Provider | Calls | Tokens | Cost | Errors |
|---|---|---|---|---|
{{#each providers_display}}
| {{this.name}} | -- | -- | -- | -- |
{{/each}}

**By Model:**
Show the top 10 models by call count with the same columns.

**By User:**
Show the top 10 users by call count with columns: User, Role, Calls, Tokens, Cost.

## Filtering

The dashboard and API support filtering by:

| Filter | Type | Description |
|---|---|---|
| Date range | `start_date`, `end_date` | Filter by `created_at` range |
| Provider | `provider` | Single or multi-select from: {{#each providers_display}}`{{this.id}}`{{#unless @last}}, {{/unless}}{{/each}} |
| Model | `model` | Single or multi-select (populated dynamically from data) |
| User | `user_id` | Filter to a specific user |
| Status | `status` | `success`, `error`, `timeout`, or all |
| Template | `prompt_template_id` | Filter by specific template |

### Preset Date Ranges

Provide quick-select buttons: Today, Last 7 Days, Last 30 Days, This Month, Last Month, Custom Range.

## Cost Tracking

### How Costs Are Calculated

The LLM Gateway calculates `estimated_cost` at the time of each call using the formula:

```
estimated_cost = (input_tokens / 1,000,000 * input_price) + (output_tokens / 1,000,000 * output_price)
```

Per-model pricing is stored in the application config (see `references/llm-pricing-table.md`). Costs are estimates -- actual billing from providers may differ slightly due to rounding.

### Cost Dashboard Elements

1. **Total cost for selected period** -- displayed prominently as a summary card
2. **Daily cost trend chart** -- line chart from `llm_audit_summary` showing cost per day
3. **Cost by provider** -- pie or bar chart showing cost distribution
4. **Cost by model** -- bar chart showing which models cost the most
5. **Cost by user** -- table showing top users by cost (helps identify outliers)
6. **Projected monthly cost** -- extrapolate current usage to estimate the full month

### Cost Alerts (Recommended)

While not required for the initial implementation, consider adding threshold alerts:
- Daily cost exceeds $X
- Single user exceeds $Y in a day
- Error rate exceeds Z%

These can be implemented as a background check that runs on each audit log insert.

## Per-User Usage Tracking

Each authenticated user can view their own LLM usage on a personal usage page (e.g., `/dashboard/usage`). This page does not require any special permission -- the `audit_select_own` RLS policy restricts the query to the user's own rows.

### User Usage Page

- **My Total Calls** / **My Total Tokens** / **My Total Cost** summary cards
- **My Recent Calls** table: date, template used (or "ad-hoc"), model, tokens, cost, status
- Paginated, most recent first
- Click a row to see details (model, latency, error message if failed)

This page gives users visibility into their own LLM consumption without needing admin access.

## Error Log Review

The admin dashboard includes an error-focused view for debugging failed LLM calls.

### Error Log View

Filter the audit log to `status IN ('error', 'timeout')` and display:

| Column | Description |
|---|---|
| Timestamp | When the error occurred |
| User | Who triggered the call |
| Provider | Which provider failed |
| Model | Which model was requested |
| Status | `error` or `timeout` |
| Error Message | The captured error detail |
| Latency | How long before the error (useful for timeout diagnosis) |
| Template | Which template was used (if any) |

### Error Patterns to Watch For

- **Repeated timeouts** on a specific model -- may indicate provider issues
- **Authentication errors** -- API key may be invalid or expired
- **Rate limit errors** -- may need to implement queuing or backoff
- **Model not found** -- template references a deprecated model

## Charts and Visualizations

Recommended charts for the audit dashboard (implement using a charting library like Recharts, Chart.js, or similar):

### 1. Daily Usage Over Time (Line Chart)
- X-axis: date
- Y-axis: request count (left) and cost (right)
- Source: `llm_audit_summary` view
- One line per provider{{#if has_multiple_providers}} to compare usage across providers{{/if}}

### 2. Token Distribution (Stacked Bar Chart)
- X-axis: date
- Y-axis: token count
- Stacked bars: input tokens vs. output tokens
- Source: `llm_audit_summary` view

### 3. Cost by Provider (Pie/Donut Chart)
- Segments: one per provider
- Values: total cost for the selected period
- Source: `llm_audit_summary` view grouped by provider

### 4. Latency Distribution (Histogram)
- X-axis: latency buckets (0-500ms, 500-1000ms, 1-2s, 2-5s, 5s+)
- Y-axis: request count
- Source: `llm_audit_log` table
- Helps identify performance outliers

### 5. Error Rate Over Time (Line Chart)
- X-axis: date
- Y-axis: error percentage
- Source: `llm_audit_summary` view (error_count / request_count)

## Export to CSV

Users with the `export_audit_log` permission can export filtered audit data to CSV.

### Export Endpoint: `GET /api/admin/audit/export`

**Query parameters:** Same filters as the dashboard (date range, provider, model, user, status).

**Response:** `Content-Type: text/csv` with `Content-Disposition: attachment; filename="llm-audit-export-YYYY-MM-DD.csv"`

**CSV columns:**
```
id,timestamp,user_email,provider,model,input_tokens,output_tokens,total_tokens,estimated_cost,latency_ms,status,error_message,template_slug
```

### Export Implementation Notes

- Stream the CSV response for large datasets (do not load all rows into memory)
- Limit exports to a maximum of 100,000 rows per request
- Include a header row
- Format timestamps as ISO 8601
- Format cost as decimal with 6 places (e.g., `0.003215`)
- Join with `user_profiles` for email and `prompt_templates` for slug
- Log the export action itself (who exported, when, what filters were applied)

## API Endpoints

### `GET /api/admin/audit`

List audit log entries with filtering and pagination.

**Query parameters:**
- `start_date` -- ISO 8601 date (inclusive)
- `end_date` -- ISO 8601 date (inclusive)
- `provider` -- filter by provider
- `model` -- filter by model
- `user_id` -- filter by user
- `status` -- filter by status
- `template_id` -- filter by prompt template
- `page` -- page number (default 1)
- `per_page` -- items per page (default 50, max 200)

**Response:**
```json
{
  "entries": [
    {
      "id": "uuid",
      "user_id": "uuid",
      "user_email": "user@example.com",
      "user_name": "John Doe",
      "prompt_template_id": "uuid",
      "template_slug": "summarize-article",
      "provider": "{{providers_display.[0].id}}",
      "model": "gpt-4o",
      "input_tokens": 142,
      "output_tokens": 287,
      "total_tokens": 429,
      "estimated_cost": 0.003215,
      "latency_ms": 1823,
      "status": "success",
      "error_message": null,
      "created_at": "2025-01-20T14:22:00Z"
    }
  ],
  "total": 15234,
  "page": 1,
  "per_page": 50
}
```

### `GET /api/admin/audit/summary`

Get aggregated metrics for the dashboard summary cards and charts.

**Query parameters:** `start_date`, `end_date`, `provider`, `model`, `user_id`

**Response:**
```json
{
  "totals": {
    "calls": 15234,
    "input_tokens": 4521000,
    "output_tokens": 2103000,
    "total_tokens": 6624000,
    "total_cost": 48.52,
    "avg_latency_ms": 1420,
    "error_count": 87,
    "error_rate": 0.0057
  },
  "by_day": [
    {
      "day": "2025-01-20",
      "request_count": 523,
      "total_tokens": 234000,
      "total_cost": 1.82,
      "error_count": 3
    }
  ],
  "by_provider": [
    {{#each providers_display}}
    {
      "provider": "{{this.id}}",
      "request_count": 0,
      "total_tokens": 0,
      "total_cost": 0
    }{{#unless @last}},{{/unless}}
    {{/each}}
  ],
  "by_model": [],
  "by_user": []
}
```

### `GET /api/admin/audit/export`

Export audit data to CSV. See the "Export to CSV" section above for details.

**Requires:** `export_audit_log` permission.

### `GET /api/user/audit` (Personal Usage)

List the current user's own audit entries. No special permission required -- RLS handles access control.

**Query parameters:** Same as `/api/admin/audit` except no `user_id` (always scoped to the authenticated user).

## Admin UI Pages

### Audit Dashboard (`/admin/audit`)

Layout:

1. **Date range selector** with preset buttons at the top
2. **Filter bar** with provider, model, user, and status dropdowns
3. **Summary cards row** -- Total Calls, Total Tokens, Total Cost, Error Rate, Avg Latency
4. **Charts section** -- Daily Usage line chart and Cost by Provider pie chart side by side
5. **Log table** -- paginated list of individual audit entries
6. **Export button** in the top-right corner

### Error Review (`/admin/audit?status=error`)

Same page as the main dashboard but pre-filtered to errors and timeouts. The log table highlights error messages in red.

### User Usage (`/dashboard/usage`)

Simplified version of the dashboard scoped to the current user. No admin permission required.

## Implementation Checklist

1. Create `GET /api/admin/audit` with filtering and pagination
2. Create `GET /api/admin/audit/summary` with aggregated metrics
3. Create `GET /api/admin/audit/export` with CSV streaming
4. Create `GET /api/user/audit` for personal usage
5. Add permission middleware (`view_audit_log` for admin routes, `export_audit_log` for export)
6. Build the dashboard page with summary cards
7. Integrate a charting library and build the 5 recommended charts
8. Build the filterable, paginated log table
9. Build the CSV export with proper headers and streaming
10. Build the personal usage page (`/dashboard/usage`)
11. Add error-focused filtering view
12. Test with various date ranges, filters, and export sizes
